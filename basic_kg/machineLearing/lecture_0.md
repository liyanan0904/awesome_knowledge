# 优化方法
机器学习本质是建立优化模型 通过优化方法堆目标函数(或损失函数)进行优化。常见的优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度法等。
## 梯度下降法
### 优化思想
当目标函数是凸函数时，梯度下降法的解是全局最优解，梯度下降法的速度未必是最快的。梯度下降法的优化思想是用当前位置的负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是“最速下降法”。最速下降法越接近目标值，步长越小，前进越慢。
### 缺点
梯度下降法的最大问题是会陷入局部最优，靠近极小值时收敛速度减慢。
### 批量梯度下降法
最小化所有训练样本的损失函数，使得最终求解的是全局最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。
### 随机梯度下降法
最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。

随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。

### 牛顿法
牛顿法是一种在实数域和复数域上近似求解方程的方法。使用韩式f(x)的泰勒级数的前面几项来寻找方程f(x)=0的根。牛顿法最大的特点就在于它的收敛速度很快。 迭代公式如下

$$x_{n+1} = x_{n} - {{f(x_{n})} \over {f'(x_{n})}} $$

### 牛顿法比梯度下降法快
牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。
但是牛顿法要算hessian矩阵的逆，比较费时间。

### 拟牛顿法
拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。

### 拉格朗日法
#### 拉格朗日乘数法
拉格朗日乘子法主要用于解决约束优化问题，它的基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题。拉格朗日乘子背后的数学意义是其为约束方程梯度线性组合中每个向量的系数。
通过引入拉格朗日乘子建立极值条件，对n个变量分别求偏导对应了n个方程，然后加上k个约束条件（对应k个拉格朗日乘子）一起构成包含了（n+k）变量的（n+k）个方程的方程组问题，这样就能根据求方程组的方法对其进行求解。


# 机器学习算法选择
## 机器学习算法选择
随机森林平均来说最强，但也只在9.9%的数据集上拿到了第一，优点是鲜有短板。SVM的平均水平紧随其后，在10.7%的数据集上拿到第一。神经网络（13.2%）和boosting（~9%）表现不错。数据维度越高，随机森林就比AdaBoost强越多，但是整体不及SVM2。数据量越大，神经网络就越强。
## 贝叶斯
是相对容易理解的一个模型，至今依然被垃圾邮件过滤器使用。

## K近邻
分类算法
典型的例子是KNN，它的思路就是——对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。
它的特点是完全跟着数据走，没有数学模型可言。

三要素：k值的选择；距离的度量（常见的距离度量有欧式距离，马氏距离等）；分类决策规则 （多数表决规则）

#### k值的选择


k值越小表明模型越复杂，更加容易过拟合

但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类

> 所以一般k会取一个较小的值，然后用过交叉验证来确定
这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后k分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的k

#### 分类决策规则
找到最近的k个实例之后，可以计算平均值作为预测值，也可以给这k个实例加上一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）
优缺点：


#### 优点
- 思想简单 
- 可用于非线性分类
- 训练时间复杂度为O(n)
- 准确度高，对outlier不敏感
#### 缺点
- 计算量大
- 样本不平衡问题不适用
- 需要大量的内存

## KD树
KD树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索

### 构造KD树
在k维的空间上循环找子区域的中位数进行划分的过程。
假设现在有K维空间的数据集。首先构造根节点，以坐标的中位数b为切分点，将根结点对应的矩形局域划分为两个区域，区域1中,区域2中
构造叶子节点，分别以上面两个区域中的中位数作为切分点，再次将他们两两划分，作为深度1的叶子节点，（如果a2=中位数，则a2的实例落在切分面）
不断重复2的操作，深度为j的叶子节点划分的时候，索取的 的，直到两个子区域没有实例时停止。

### KD树的搜索

首先从根节点开始递归往下找到包含x的叶子节点，每一层都是找对应的$x_{i}$
将这个叶子节点认为是当前的“近似最近点”,
递归向上回退，如果以x圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与x更近的点，则进入另一个子区域中查找该点并且更新”近似最近点“
重复3的步骤，直到另一子区域与球体不相交或者退回根节点
最后更新的”近似最近点“与x真正的最近点

## 决策树
决策树的特点是它总是在沿着特征做切分。随着层层递进，这个划分会越来越细。因为它能够生成清晰的基于特征(feature)选择不同预测结果的树状结构

## 随机森林

### RF与传统bagging的区别
- 样本采样：RF有放回选取和整体样本数目相同的样本，一般bagging用的样本<总体样本数
- 特征采样：RF对特征进行采样，bagging用全部特征
### RF的优点
- 在数据集上表现良好，在当先很多数据集上要优于现有的很多算法
- 可以并行，且不是对所有属性进行训练，训练速度相对较快
- 防止过拟合
- 能够处理高维特征，且不用做特征选择，可以给出特征重要性的评分，训练过程中，可以检测到feature的相互影响
### 缺点
- 树越多，随机森林的表现才会越稳定。所以在实际使用随机森林的时候需要注意如果树不够多的时候，可能会导致不稳定的情况。
- 不平衡数据集。分类结果会倾向于样本多的类别，所以训练样本中各类别的数据必须相同。Breiman在实际实现该算法的时候有考虑到了这个问题，采取了根据样本类别比例对决策树的判断赋予不同权值的方法
### RF的学习算法
- ID3：离散
- C4.5：连续
- CART：离散或连续
## GBDT
GBDT（梯度迭代决策树）是一种基于决策回归树的Boosting模型，其核心思想是将提升过程建立在对“之前残差的负梯度表示”的回归拟合上，通过不断的迭代实现降低偏差的目的。



GBDT设置大量基学习器的目的是为了集成来降低偏差，所以 n_estimators （基决策器的个数）一般会设置得大一些。



对于GBDT模型来说，其每个基学习器是一个弱学习器(欠拟合)，决策树的深度一般设置得比较小，以此来降低方差（模型复杂度低），之后在经过残差逼近迭代来降低偏差，从而形成强学习器。
### GBDT与传统Boosting的区别
Boosting算法，但与传统boosting有区别、拟合上一步的残差，传统意义上说不能并行，只能用CART回归树，降低偏差
迭代思路不同：传统boosting对训练样本进行加权，GBDT则是拟合残差，下一棵树沿残差梯度下降的方向进行拟合
### GBDT正则化的方式
- 同AdaBoost，通过步长
- CART树的剪枝
- 子抽样，不放回，SGBT，可以实现一定程度上的并行

### GBDT的优缺点
* 优点
1. 调参少的情况下，准确率也高（SVM）
2. 灵活处理各种数据，包括连续和离散，无需归一化处理（LR）
3. 模型非线性变换多，特征不用经过复杂处理即可表达复杂信息
4. 从一定程度上可以防止过拟合，小步而非大步拟合
* 缺点
1. 一般来说传统的GBDT只能串行，但是也可以通过子采样比例（0.5~0.8）实现某种意义上的并行，但一般这就不叫GBDT了。
2. 对异常值敏感，但是可以采取一些健壮的损失函数缓解，如Huber./Quantile损失函数
#### GBDT预测时每一棵树是否能并行？
可以，训练需串行，预测可并行

### GBDT和RF的区别与联系
- 联系：多棵树进行训练+多棵树共同进行预测
- 区别：
1. 取样方式
2. 预测时，RF多数投票，GBDT加权累加
3. 样本的关系—>并行和串行
4. 学期器的种类，GBDT只能用CART回归树 (因为要计算连续梯度)
5. 对异常值的敏感性
6. 通过减少方差/偏差提高性能

### XGBOOST相比于GBDT有何不同？XGBOOST为什么快？XGBOOST如何支持并行？ 
[机器学习算法中 GBDT 和 XGBOOST 的区别有哪些](https://www.zhihu.com/question/41354392/answer/98658997)
1. 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
2. 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。
3. xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
4. Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
5. 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
6. 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。
7. xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
8. 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。


### ababoost


adaBoost的优缺点
优点：
1. 容易理解、实现简单
2. 易编码
3. 分类精度高
4. 可以使用各种回归模型构建基分类器，非常灵活
5. 作为二元分类器是，构造简单、结果可理解、少参数
6. 相对来说，不宜过拟合
缺点：
1. 只能串行
2. 对异常值敏感 boosting对异常值敏感

##  集成学习与方差偏差
我觉得，避免偏差的话，首先我们需要尽量选择正确的模型，所谓“对症下药”。我觉得有位同行把机器学习算法的使用比作医生开药方，是非常不错的比喻。我们要根据数据的分布和特点，选择合适的算法。

其次，有了合适的算法，我们还要慎重选择数据集的大小。通常训练数据集越大越好，但是当大到数据集已经对整体所有数据有了一定的代表性之后，再多的数据已经不能提升模型的准确性，反而带来模型训练的计算量增加。但是，训练数据太少的话是一定不好的，这会带来过拟合的问题，过拟合就是模型复杂度太高，方差很大，不同的数据集训练出来的模型变化非常大.

### 为什么说bagging是减少variance，而boosting是减少bias?
从机制上讲 [为什么说bagging是减少variance，而boosting是减少bias](https://www.zhihu.com/question/26760839) 

## 分类与回归的区别
分类和回归的区别在于输出变量的类型
- 定量输出称为回归，或者说是连续变量预测；
- 定性输出称为分类，或者说是离散变量预测。
## 生成模型与判别模型的区别
有监督机器学习方法可以分为生成方法和判别方法（常见的生成方法有混合高斯模型、朴素贝叶斯法和隐形马尔科夫模型等，常见的判别方法有SVM、LR等），生成方法学习出的是生成模型，判别方法学习出的是判别模型。

监督学习，预测时，一般都是在求p(Y|X)
- 生成模型： 从数据中学习联合概率分布p(X,Y)，然后利用贝叶斯公式求：
- 判别模型：直接学习P(Y|X)， 它直观输入什么特征X，就直接预测出最可能的Y; 典型的模型包括：LR, SVM,CRF,Boosting,Decision tree....

1. 生成方法的特点：生成方法可以还原联合概率分布，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学习的模型可以更快的收敛于真实的模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。
2. 判别方法的特点：判别方法直接学习的是条件概率或者决策函数，直接面对预测，往往学习的准确率更高；由于直接学习或者，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。




## 线性分类器与非线性分类器的区别以及优劣
如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。
常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归
常见的非线性分类器：决策树、RF、GBDT、多层感知机
SVM两种都有(看线性核还是高斯核)

## 样本不均衡如何解决

主要三个方面，数据，模型和评估方法。

数据上重采样和欠采样，使之均衡；

模型上选对样本不均衡问题不敏感的模型，和算法集成技术，如决策树，不能用KNN；

评估方法，用查全率，查准率之类

## 重采样（resampling）技术：

(1). 随机欠采样
随机欠采样的目标是通过随机地消除占多数的类的样本来平衡类分布。
优点
它可以提升运行时间；并且当训练数据集很大时，可以通过减少样本数量来解决存储问题。
缺点
它会丢弃对构建规则分类器很重要的有价值的潜在信息。
被随机欠采样选取的样本可能具有偏差。它不能准确代表大多数。



(2). 随机过采样（Random Over-Sampling）
过采样（Over-Sampling）通过随机复制少数类来增加其中的实例数量，从而可增加样本中少数类的代表性。
优点
与欠采样不同，这种方法不会带来信息损失。
表现优于欠采样。
缺点
由于复制少数类事件，它加大了过拟合的可能性。



(3). 信息性过采样：合成少数类过采样技术
直接复制少数类实例并将其添加到主数据集时。从少数类中把一个数据子集作为一个实例取走，接着创建相似的新合成的实例。这些合成的实例接着被添加进原来的数据集。新数据集被用作样本以训练分类模型。
优点
通过随机采样生成的合成样本而非实例的副本，可以缓解过拟合的问题。
不会损失有价值信息。
缺点
当生成合成性实例时，SMOTE 并不会把来自其他类的相邻实例考虑进来。这导致了类重叠的增加，并会引入额外的噪音。






























